---
pagetitle: GWAS
---

# Prepare 1000G data


```{r}
# load libraries
library(tidyverse)
library(janitor)
```


## PLINK2 files

We use the PLINK files provided by the PLINK2 developers:

```bash
wget -O data/plink/all_hg38.pgen.zst "https://www.dropbox.com/s/j72j6uciq5zuzii/all_hg38.pgen.zst?dl=1"
wget -O data/plink/all_hg38.pvar.zst "https://www.dropbox.com/scl/fi/id642dpdd858uy41og8qi/all_hg38_rs_noannot.pvar.zst?rlkey=sskyiyam1bsqweujjmxqv1h55&dl=1"
wget -O data/plink/all_hg38.psam "https://www.dropbox.com/s/gyobtdi904m9bir/hg38_orig.psam?dl=1"

unzstd --rm data/plink/all_hg38.pgen.zst
unzstd --rm data/plink/all_hg38.pvar.zst
```

## Filter samples

```{r}
psam <- read_tsv("data/plink/all_hg38.psam") |> 
  clean_names(replace = c("#" = ""))

# filter samples and save to later filter with plink2
set.seed(20250225)
sample_info <- psam |> 
  # unrelated individuals only
  filter(pat == 0 & mat == 0) |> 
  # sample 40% of individuals
  slice_sample(prop = 0.4)

sample_info |> 
  select(iid) |> 
  write_tsv("data/plink/subset_sample_ids.tsv", 
            col_names = FALSE)
```


## Filter SNPs

```{r}
snps <- read_tsv("data/plink/all_hg38.pvar", 
                 comment = "##", lazy = TRUE) |> 
  clean_names(replace = c("#" = ""))

# filter SNPs
# retain only standard chromosomes, including PARs
# only SNPs (no indels) with standard rs id
# biallelic SNPs (remove duplicates)
# variants occurring in multiple locations
snps <- snps |> 
  filter(chrom %in% c(1:22, "PAR1", "PAR2", "X", "Y") & 
         ref %in% c("A", "C", "T", "G") & 
         alt %in% c("A", "C", "T", "G") &
         str_detect(id, "^rs") & 
         vctrs::vec_duplicate_detect(paste(chrom, pos)) &
         vctrs::vec_duplicate_detect(id))
```


## Trait SNPs

To simulate traits, we exported summary tables from GWAS catalogue: 

- Binary trait: type 2 diabetes; study accession [GCST006801](https://www.ebi.ac.uk/gwas/studies/GCST006801).
- Binary trait: chronotype ("morning person"); study accession [GCST007565](https://www.ebi.ac.uk/gwas/studies/GCST007565). 
- Continuous trait: caffeine consumption ("mg/day"); study accession [GCST001032](https://www.ebi.ac.uk/gwas/studies/GCST001032).
- Continuous trait: pulse pressure measurement (mm Hg), i.e. the difference in pulse pressure between systolic and diastolic pressure; study accession [GCST001235](https://www.ebi.ac.uk/gwas/studies/GCST001235)

We now merge this back to the `snps` table for each trait.


### Type 2 diabetes

```{r}
# read gwas catalogue data
t2d <- read_tsv("data/gwas_catalogue/t2d_GCST006801.tsv",
                       na = "") |> 
  clean_names()

t2d <- t2d |> 
  select(chrom = chr_id, pos = chr_pos, effect = or_or_beta, 
         risk_allele = strongest_snp_risk_allele) |> 
  mutate(chrom = as.character(chrom)) |> 
  separate(risk_allele, 
           into = c("id", "risk_allele"), 
           sep = "-")

# collapse linked SNPs
t2d <- t2d |> 
  arrange(chrom, pos) |> 
  group_by(chrom) |> 
  # this is so smart! Thanks ChatGPT
  mutate(cluster = cumsum(c(1, diff(pos) > 250e3))) |> 
  group_by(chrom, cluster) |> 
  slice_max(effect) |> 
  ungroup() |> 
  select(id, effect, risk_allele)

# filtered SNPs
t2d_snps <- snps |> 
  inner_join(t2d, 
             by = "id")

# adjust effect sizes
t2d_snps <- t2d_snps |> 
  mutate(effect = ifelse(ref == risk_allele, 
                         1/effect, effect))

# write to file
t2d_snps |> 
  write_tsv("data/gwas_catalogue/t2d_effects.tsv")
```


### Chronotype

```{r}
# read gwas catalogue data
chronotype <- read_tsv("data/gwas_catalogue/chronotype_GCST007565.tsv",
                       na = "") |> 
  clean_names()

chronotype <- chronotype |> 
  select(chrom = chr_id, pos = chr_pos, effect = or_or_beta, 
         risk_allele = strongest_snp_risk_allele) |> 
  mutate(chrom = as.character(chrom)) |> 
  separate(risk_allele, 
           into = c("id", "risk_allele"), 
           sep = "-")

# collapse linked SNPs
chronotype <- chronotype |> 
  arrange(chrom, pos) |> 
  group_by(chrom) |> 
  # this is so smart! Thanks ChatGPT
  mutate(cluster = cumsum(c(1, diff(pos) > 250e3))) |> 
  group_by(chrom, cluster) |> 
  slice_max(effect) |> 
  ungroup() |> 
  select(id, effect, risk_allele)

# filtered SNPs
chronotype_snps <- snps |> 
  inner_join(chronotype, by = "id")

# adjust effect sizes
chronotype_snps <- chronotype_snps |> 
  mutate(effect = ifelse(ref == risk_allele, 
                         1/effect, effect))

# write to file
chronotype_snps |> 
  write_tsv("data/gwas_catalogue/chronotype_effects.tsv")
```

### Caffeine consumption

```{r}
# read gwas catalogue data
coffee <- read_tsv("data/gwas_catalogue/coffee_GCST001032.tsv",
                       na = "") |> 
  clean_names()

coffee <- coffee |> 
  select(chrom = chr_id, pos = chr_pos, effect = or_or_beta, 
         risk_allele = strongest_snp_risk_allele) |> 
  mutate(chrom = as.character(chrom)) |> 
  separate(risk_allele, 
           into = c("id", "risk_allele"), 
           sep = "-")

# collapse linked SNPs
coffee <- coffee |> 
  arrange(chrom, pos) |> 
  group_by(chrom) |> 
  # this is so smart! Thanks ChatGPT
  mutate(cluster = cumsum(c(1, diff(pos) > 250e3))) |> 
  group_by(chrom, cluster) |> 
  slice_max(effect) |> 
  ungroup() |> 
  select(id, effect, risk_allele)

# filtered SNPs
coffee_snps <- snps |> 
  inner_join(coffee, by = "id")

# adjust effect sizes
coffee_snps <- coffee_snps |> 
  mutate(effect = ifelse(ref == risk_allele, 
                         -effect, effect))

# write to file
coffee_snps |> 
  write_tsv("data/gwas_catalogue/coffee_effects.tsv")
```

### Blood pressure

```{r}
# read gwas catalogue data
blood <- read_tsv("data/gwas_catalogue/blood_GCST001235.tsv",
                       na = "") |> 
  clean_names()

blood <- blood |> 
  select(chrom = chr_id, pos = chr_pos, effect = or_or_beta, 
         risk_allele = strongest_snp_risk_allele) |> 
  mutate(chrom = as.character(chrom)) |> 
  separate(risk_allele, 
           into = c("id", "risk_allele"), 
           sep = "-")

# collapse linked SNPs
blood <- blood |> 
  arrange(chrom, pos) |> 
  group_by(chrom) |> 
  # this is so smart! Thanks ChatGPT
  mutate(cluster = cumsum(c(1, diff(pos) > 250e3))) |> 
  group_by(chrom, cluster) |> 
  slice_max(effect) |> 
  ungroup() |> 
  select(id, effect, risk_allele)

# filtered SNPs
blood_snps <- snps |> 
  inner_join(blood, by = "id")

# adjust effect sizes
blood_snps <- blood_snps |> 
  mutate(effect = ifelse(ref == risk_allele, 
                         -effect, effect))

# write to file
blood_snps |> 
  write_tsv("data/gwas_catalogue/blood_effects.tsv")
```


## Subset SNPs

We now subsample the full list of SNPs to a manageable size for teaching.
One thing we need to ensure is that the causal SNPs for our traits are included. 
To make the regional plots more interesting, we sample SNPs densely around the causal SNPs.
We do this by performing an interval join 500Kb around each causal SNP.
The disadvantage of this is that it causes some distortions in the p-value histogram and Q-Q plot, but nothing major.

We start by creating intervals for each SNP:

```{r}
causal_intervals <- bind_rows(chronotype_snps, coffee_snps, 
                    blood_snps, t2d_snps) |> 
  mutate(start = pos - 250e3, end = pos + 250e3) |> 
  select(chrom, start, end)
```

We then create an interval join between this table and the full SNPs table:

```{r}
causal_snps <- snps |> 
  select(chrom, pos, id) |> 
  inner_join(causal_intervals, 
             by = join_by(chrom, between(pos, start, end))) |> 
  pull(id)
``` 

Then, we randomly sample 5% of the SNPs, and join back this set of SNPs:

```{r}
set.seed(20250225)
# faster than using dplyr::slice_sample
snps_sample <- sample(snps$id, round(0.05*nrow(snps)))

# bring back the causal SNPs
snps_sample <- unique(c(snps_sample, causal_snps))
```

Visualise the SNPs, as a sanity check:

```{r}
snps |> 
  filter(id %in% snps_sample) |>
  count(chrom) |> 
  ggplot(aes(n, chrom)) + geom_col()
  
snps |> 
  filter(id %in% snps_sample) |>
  slice_sample(n = 10000) |> 
  mutate(y = rnorm(n())) |> 
  ggplot(aes(pos, y)) +
  geom_point(shape = "|") +
  facet_grid( ~ chrom, scale = "free_x", space = "free_x")
  
snps |> 
  filter(id %in% snps_sample) |>
  slice_sample(n = 1000e3) |> 
  ggplot(aes(pos/1e6, chrom)) +
  geom_bin_2d(binwidth = c(1, 1)) +
  theme_classic()
```

Finally, we write this list of SNPs to a file to later subset the VCF files with `plink2`:

```{r}
tibble(id = snps_sample) |> 
  write_tsv("data/plink/subset_snp_ids.tsv", 
            col_names = FALSE)
```


## Extract genotypes for causal SNPs

We convert the original PLINK files to VCF, for convenience of extracting genotypes and later adding missing data with a custom script. 

```bash
# convert to VCF
plink2 \
  --pfile data/plink/all_hg38 \
  --export vcf bgz \
  --keep data/plink/subset_sample_ids.tsv \
  --extract data/plink/subset_snp_ids.tsv \
  --out data/vcf/1000G_subset

bcftools index data/vcf/1000G_subset.vcf.gz
```

Extract the genotypes of target (causal) variants into a TSV file, which will be used to simulate our traits: 

```bash
for trait in chronotype t2d coffee blood 
do 
  echo "Extracting genotypes for $trait"
  
  # create file headers
  samples=$(bcftools query -l data/vcf/1000G_subset.vcf.gz | tr '\n' '\t' | sed 's/\t$//')
  printf "snp\t$samples\n" > data/gwas_catalogue/${trait}_geno.tsv

  # temporary regions file from SNP effects file
  cut -f 1,2 data/gwas_catalogue/${trait}_effects.tsv | \
    tail -n +2 \
    > temp_${trait}_snps.tsv
  
  # create TSV file numerically coding genotypes as 0/1/2
  bcftools query \
    -f '%CHROM\t%POS[\t%GT]\n' \
    --regions-file temp_${trait}_snps.tsv \
    data/vcf/1000G_subset.vcf.gz |\
    # replace the first tab with underscore (to get joint SNP id)
    sed 's/\t/_/' |\
    # recode genotypes as numeric
    sed 's/0|0/0/g' | sed 's/1|0/1/g' | sed 's/0|1/1/g' | sed 's/1|1/2/g' \
    >> data/gwas_catalogue/${trait}_geno.tsv
  
  # clean up temp file
  rm temp_${trait}_snps.tsv
done
```

## Simulating traits

Now we have the pieces to simulate our traits: 

- The genotypes
- The effect sizes

The following is a function to simulate a trait based on a genotype matrix and effect size vector.

```{r}
#' Simulate a Trait Based on Genotype Matrix and Effect Sizes
#'
#' This function simulates a quantitative or binary trait based on a given genotype matrix, 
#' effect sizes, and heritability. Optionally, it can simulate a binary trait by specifying 
#' a prevalence.
#'
#' @param genotype_matrix A numeric matrix where rows represent individuals and columns 
#'        represent genetic variants (e.g., SNPs). Each entry corresponds to the genotype 
#'        value for a given individual and variant.
#' @param effect_sizes A numeric vector of effect sizes corresponding to the genetic variants. 
#'        The length of this vector must match the number of columns in `genotype_matrix`.
#' @param h2 A numeric value representing the heritability of the trait. Must be between 0 and 1. 
#'        Default is 0.5.
#' @param prevalence A numeric value representing the prevalence of the trait in the population. 
#'        If specified, the function simulates a binary trait. Must be between 0 and 1. 
#'        Default is `NULL`, which simulates a quantitative trait.
#'
#' @return A numeric vector representing the simulated trait values for each individual. 
#'         If `prevalence` is specified, the vector contains binary values (0 or 1).
#'
#' @examples
#' # Simulate a quantitative trait
#' genotype_matrix <- matrix(rbinom(1000, 2, 0.5), nrow = 100, ncol = 10)
#' effect_sizes <- runif(10, -1, 1)
#' simulated_trait <- simulate_trait(genotype_matrix, effect_sizes, h2 = 0.7)
#'
#' # Simulate a binary trait
#' simulated_binary_trait <- simulate_trait(genotype_matrix, effect_sizes, h2 = 0.7, prevalence = 0.1)
#'
#' @export
simulate_trait <- function(genotype_matrix, 
                           effect_sizes, 
                           h2 = 0.5, 
                           prevalence = NULL) {
  
  # Sanity checks
  if (!is.matrix(genotype_matrix) || !is.numeric(genotype_matrix)) 
    stop("genotype_matrix must be a numeric matrix.")
  
  if (length(effect_sizes) != ncol(genotype_matrix)) 
    stop("Length of effect_sizes must match the number of columns in genotype_matrix.")
  
  if (!is.numeric(h2) || h2 <= 0 || h2 > 1) 
    stop("h2 must be a numeric value between 0 and 1.")
  
  if (!is.null(prevalence) && (!is.numeric(prevalence) || prevalence <= 0 || prevalence >= 1)) 
    stop("prevalence must be a numeric value between 0 and 1 if specified.")
  
  if (!is.null(prevalence)) {
    # transform OR to liability scale, which is not trivial
    # see https://doi.org/10.1007/s10519-021-10042-2
    # using Pawitan transformation from that paper
    p = colSums(genotype_matrix)/(nrow(genotype_matrix)*2)
    var = 2*p*(1-p)*log(effect_sizes)^2
    var = var/(var + pi^2 / 3)
    effect_sizes <- ifelse(log(effect_sizes) > 0, sqrt(var), -sqrt(var))
  }
  
  # Compute genetic component
  genetic_component <- as.numeric(genotype_matrix %*% effect_sizes)
  var_g <- var(genetic_component) # genetic variance
  
  # Scale genetic component to desired proportion of total variance
  genetic_component <- genetic_component / sqrt(var_g) * sqrt(h2)
  
  # Environmental noise to explain remaining variance (1 - h2)
  environmental_noise <- rnorm(nrow(genotype_matrix), 
                               mean = 0, 
                               sd = sqrt(1 - h2))
  
  # Compute trait value (equivalent to "liability" for a binary trait)
  trait_values <- genetic_component + environmental_noise
  
  # Convert to binary trait if prevalence is provided
  if (!is.null(prevalence)) {
    trait_values <- (trait_values - mean(trait_values))/sd(trait_values)
    threshold <- qnorm(1 - prevalence)
    return(as.numeric(trait_values > threshold))
  }
  
  return(as.numeric(trait_values))
}
```

Here we simulate data for one of the traits, but later we loop through all traits.

```{r}
# effect sizes
effects <- read_tsv("data/gwas_catalogue/blood_effects.tsv") |> 
  mutate(snp = paste(chrom, pos, sep = "_")) |> 
  select(snp, effect) |> 
  column_to_rownames("snp") |> 
  as.matrix()

genos <- read_tsv("data/gwas_catalogue/blood_geno.tsv", 
                  col_types = cols(snp = "c", .default = "i"))

# convert to transposed matrix
genos <- genos |> column_to_rownames("snp") |> as.matrix() |> t()

# simulate trait
trait <- simulate_trait(genos, effects[, 1], h2 = 0.1)

# visual sanity check
cbind(genos, trait = trait) |> 
  as_tibble(rownames = "sample") |> 
  pivot_longer(matches("_"), names_to = "snp", values_to = "genotype") |> 
  group_by(snp, genotype) |> 
  summarise(trait_mean = mean(trait), 
            trait_sd = sd(trait)) |> 
  ungroup() |> 
  ggplot(aes(factor(genotype), trait_mean)) +
  geom_line(aes(group = snp))
```

Run simulation for all traits:

```{r}
sim_config <- tibble(
  trait = c("coffee", "blood", "chronotype", "t2d"),
  h2 = c(0.5, 0.2, 0.3, 0.4),
  prevalence = c(NA, NA, 0.15, 0.2)
)

# loop through
set.seed(20250227)
sim_traits <- pmap(sim_config, function(trait, h2, prevalence){

  # effect sizes
  effects <- read_tsv(paste0("data/gwas_catalogue/", trait, "_effects.tsv")) |> 
    mutate(snp = paste(chrom, pos, sep = "_")) |> 
    pull(effect)

  genos <- read_tsv(paste0("data/gwas_catalogue/", trait, "_geno.tsv"), 
                    col_types = cols(snp = "c", .default = "i"))

  # convert to transposed matrix
  genos <- genos |> column_to_rownames("snp") |> as.matrix() |> t()

  # simulate trait
  if (is.na(prevalence)) {
    sim <- simulate_trait(genos, effects, h2 = h2)
  } else {
    sim <- simulate_trait(genos, effects, h2 = h2, prevalence = prevalence)
  }
  
  out <- tibble(iid = rownames(genos))
  # randomly add some missing values
  out[[trait]] <- ifelse(runif(length(sim)) < 0.01, NA, sim)
    
  return(out)
})
names(sim_traits) <- sim_config$trait

pheno <- sim_traits |> 
  reduce(full_join, by = "iid") |> 
  # scale up continuous traits to realistic values
  mutate(coffee = 130 + coffee * 20,
         blood = 45 + blood * 5)

# visual sanity check
pheno |> 
  pivot_longer(-iid, 
               names_to = "trait", 
               values_to = "value") |> 
  ggplot(aes(value)) + 
  geom_histogram() +
  facet_grid(~ trait, scale = "free_x")
```

Finally, we write a PLINK-compatible phenotype file. 
We also add a sex-effect for coffee consumption, and recode the binary traits to 1/2 (PLINK standard). 

```{r}
sample_info |> 
  select(iid, sex) |> 
  full_join(pheno, by = "iid") |> 
  rename(IID = iid) |> 
  # recode cases/controls as 1/2, which PLINK uses
  mutate(chronotype = chronotype + 1, 
         t2d = t2d + 1) |> 
  # add sex effect to coffee
  mutate(coffee = coffee + 10 * (sex == 1)) |> 
  select(-sex) |> 
  write_tsv("data/phenotypes.tsv")
```

## Convert VCF to plink

Before converting our VCF, we set some genotypes to missing randomly (for teaching purposes).
We use a custom python script for this, which does the following:

- 20% of samples have some missing genotypes.
- 30% of variants have missing data.
- Missing probabilities are sampled from a Beta distribution to give a reasonably realistic distribution that tails off. 

```bash
python utils/vcf_missing.py -i data/vcf/1000G_subset.vcf.gz -o temp.vcf.gz
```

We convert our VCF file to both new and old (version 1) PLINK files. 
We retain the information about our samples by creating a temporary "psam" file from the original plink files. 

```bash
# temporary psam file
plink2 \
  --pfile data/plink/all_hg38 \
  --make-just-psam \
  --keep data/plink/subset_sample_ids.tsv \
  --out temp

# BED format for PLINK1 or software that only supports it
plink2 \
  --vcf temp.vcf.gz \
  --psam temp.psam \
  --split-par hg38 \
  --make-bed \
  --out data/plink/1000G_subset

# new format
plink2 \
  --vcf temp.vcf.gz \
  --psam temp.psam \
  --split-par hg38 \
  --make-pgen \
  --out data/plink/1000G_subset

# remove temporary files
rm temp.*
```

Finally, swap the sex of two individuals in the psam file.

```bash
# Flip the sex of a couple of individuals (12th and 23rd lines)
awk '
  BEGIN { OFS="\t" }
  NR==12 {
    if ($2==1) $2=2; 
    else if ($2==2) $2=1
  } 
  NR==23 {
    if ($2==1) $2=2; 
    else if ($2==2) $2=1
  } 
  1
' data/plink/1000G_subset.psam > temp.psam 
mv temp.psam data/plink/1000G_subset.psam
```